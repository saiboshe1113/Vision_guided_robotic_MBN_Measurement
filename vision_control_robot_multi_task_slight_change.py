# -*- coding: utf-8 -*-
import time
import math
import cv2
import numpy as np
import pyorbbecsdk as ob
import torch
import torch.nn as nn
import torchvision
import msvcrt
from collections import deque
import os  # ğŸ”¹ ç”¨äºåˆ é™¤ txt

from rtde_control import RTDEControlInterface
from rtde_receive import RTDEReceiveInterface

# =============== åŸºæœ¬é…ç½® ===============

MODEL_PATH = r"./runs_deeplab/best_deeplab_resnet50_mIoU0.9079.pth"
ROBOT_IP = "172.168.1.100"

NUM_CLASSES = 4
INPUT_SIZE = 512

# åˆ†ç±»åˆ«é˜ˆå€¼
CLS_THRESH = {1: 0.85, 2: 0.80, 3: 0.80}
CLS_AREA = {1: 1800, 2: 1300, 3: 600}

# ç›¸æœº/å·¥å…·/ä¼ æ„Ÿå™¨å‡ ä½•å…³ç³»
SAFE_OFFSET = -0.01  # è¯†åˆ«åï¼Œæ²¿å·¥å…·Zå†ä¸‹ 1cmï¼ˆå®‰å…¨ï¼‰
CAM_Z_DOWN = -0.02  # ç›¸æœºæ¯” TCP ä½ 1cm
CAM_ORI_ID = 0  # ç›¸æœºä¸å·¥å…·å§¿æ€å…³ç³»ï¼ˆç›®å‰è®¾æˆä¸€è‡´ï¼‰

ACC = 0.1
VEL = 0.1

# ğŸ”¹ Z è½´ç²—/ç»†è°ƒç”¨æ›´æ…¢çš„é€Ÿåº¦ï¼ˆå¤§èŒƒå›´ç§»åŠ¨ä»ç”¨ä¸Šé¢çš„ VEL/ACCï¼‰
VEL_Z_COARSE = 0.005  # ç²—è°ƒï¼š3 cm/s
VEL_Z_FINE = 0.001  # ç»†è°ƒï¼š1 cm/s
ACC_Z = 0.02  # Z å‘åŠ é€Ÿåº¦å°ä¸€ç‚¹ï¼Œæ›´æŸ”å’Œ

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class_names = ["BG", "sample1", "sample2", "sample3"]
class_colors = [(0, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 255)]

# ç±»é—´ä¼˜å…ˆçº§ï¼ˆç”¨äº 2 ç§’è¯†åˆ«é˜¶æ®µçš„äº’æ–¥ï¼‰
CLASS_PRIORITY = {1: 3, 2: 2, 3: 1}

# æ—¶åºå¤šæ•°æŠ•ç¥¨ï¼ˆæŠ‘åˆ¶ç¬æ—¶è¯¯æ£€ï¼‰
TEMPORAL_BUF = {1: {}, 2: {}, 3: {}}  # {cls: {track_key: (cx,cy,deque)}}
MAX_TRACKS_PER_CLASS = 8
HISTORY = 5
VOTE_K = 3

# ä¼ æ„Ÿå™¨ç›¸å¯¹å·¥å…· Y æ–¹å‘åç§» 55mm
SENSOR_OFFSET_y = -0.0875  # æ–¹å‘åäº†å°±æ”¹æˆ +0.055
SENSOR_OFFSET_X = -0.0075

# æ¯ä¸ªç±»åˆ«æœ€å¤šæµ‹å‡ ä¸ªç›®æ ‡
MAX_INST_PER_CLASS = 4  # æƒ³æµ‹å®Œå¯è®¾å¤§ä¸€ç‚¹ï¼Œä¾‹å¦‚ 99

# =============== ç²—/ç»†è°ƒ Zï¼ˆåŸºäº txtï¼‰ ===============

# ä¼ æ„Ÿå™¨è¾“å‡º txtï¼ˆæ¯æ¬¡è¿½åŠ ä¸€è¡Œï¼Œæœ€åä¸€è¡Œæ˜¯æœ€æ–°ä¿¡å·ï¼‰
TXT_PATH = r"C:/Users/Saber/Desktop/test2/test2/mbn_rms_log.txt"

# è¯»æ•°é—´éš”ï¼šç²—è°ƒ + å¾®è°ƒçº¦ 6 æ­¥ â†’ å¤§çº¦ 5â€“6 s
READ_INTERVAL = 1.5

THRESHOLD = 12.0  # ç²—/ç»†è°ƒä¿¡å·é˜ˆå€¼

# æ­¥é•¿ï¼šä» 3â€“5 mm èµ°åˆ°è¡¨é¢ï¼Œä¸€èˆ¬ 6 æ­¥å·¦å³
STEP_COARSE = -0.0005  # ç²—è°ƒ 0.8 mm æ¯æ­¥
STEP_FINE = -0.0001  # ç»†è°ƒ 0.2 mm æ¯æ­¥

DELTA_LIMIT = 0.8
N_STABLE = 3  # è¿ç»­ N æ¬¡ Î”signal å¾ˆå° â†’ åˆ°è¾¾è¡¨é¢

FINAL_HOLD_SECONDS = 5.0  # åˆ°è¾¾è¡¨é¢åä¿æŒä½ç½®å¤šå°‘ç§’é‡‡é›†æœ€ç»ˆä¿¡å·


# =============== å·¥å…·å‡½æ•° ===============

def convert_color_frame_to_bgr(color_frame):
    fmt = color_frame.get_format()
    w = color_frame.get_width()
    h = color_frame.get_height()
    data = color_frame.get_data()
    if fmt == ob.OBFormat.MJPG:
        return cv2.imdecode(np.frombuffer(data, np.uint8), cv2.IMREAD_COLOR)
    elif fmt == ob.OBFormat.YUYV:
        return cv2.cvtColor(np.frombuffer(data, np.uint8).reshape(h, w, 2),
                            cv2.COLOR_YUV2BGR_YUY2)
    arr = np.frombuffer(data, np.uint8)
    if arr.size == w * h * 3:
        return arr.reshape(h, w, 3)
    return None


def load_model():
    from torchvision.models.segmentation import deeplabv3_resnet50
    model = deeplabv3_resnet50(weights=None, aux_loss=True)
    in_ch = model.classifier[4].in_channels
    model.classifier[4] = nn.Conv2d(in_ch, NUM_CLASSES, kernel_size=1)
    if getattr(model, "aux_classifier", None) is not None:
        aux_in = model.aux_classifier[4].in_channels
        model.aux_classifier[4] = nn.Conv2d(aux_in, NUM_CLASSES, kernel_size=1)

    state = torch.load(MODEL_PATH, map_location=DEVICE)
    try:
        model.load_state_dict(state, strict=True)
        print("âœ… state_dict strict=True åŒ¹é…")
    except Exception as e:
        print(f"âš  strict=True åŠ è½½å¤±è´¥ï¼š{e}\n   å°è¯• strict=False")
        missing, unexpected = model.load_state_dict(state, strict=False)
        print(f"   missing_keys: {missing}\n   unexpected_keys: {unexpected}")

    model.to(DEVICE).eval()
    print("âœ… DeepLabv3-ResNet50 Loaded (aux_loss=True)")
    return model


SEG_MODEL = load_model()


def pose_to_matrix(pose):
    x, y, z, rx, ry, rz = pose
    theta = math.sqrt(rx * rx + ry * ry + rz * rz)
    if theta < 1e-6:
        R = np.eye(3)
    else:
        kx, ky, kz = rx / theta, ry / theta, rz / theta
        K = np.array([[0, -kz, ky], [kz, 0, -kx], [-ky, kx, 0]])
        R = np.eye(3) + math.sin(theta) * K + (1 - math.cos(theta)) * (K @ K)
    T = np.eye(4)
    T[:3, :3] = R
    T[:3, 3] = [x, y, z]
    return T


def get_cam_rotation(cam_ori_id: int):
    if cam_ori_id == 0:
        return np.eye(3)
    elif cam_ori_id == 1:
        rx = -math.pi / 2
        return np.array([[1, 0, 0], [0, math.cos(rx), -math.sin(rx)], [0, math.sin(rx), math.cos(rx)]])
    elif cam_ori_id == 2:
        rx = math.pi / 2
        return np.array([[1, 0, 0], [0, math.cos(rx), -math.sin(rx)], [0, math.sin(rx), math.cos(rx)]])
    elif cam_ori_id == 3:
        ry = math.pi / 2
        return np.array([[math.cos(ry), 0, math.sin(ry)], [0, 1, 0], [-math.sin(ry), 0, math.cos(ry)]])
    elif cam_ori_id == 4:
        ry = -math.pi / 2
        return np.array([[math.cos(ry), 0, math.sin(ry)], [0, 1, 0], [-math.sin(ry), 0, math.cos(ry)]])
    else:
        return np.eye(3)


def bbox_iou(b1, b2):
    x1, y1, w1, h1 = b1;
    x2, y2, w2, h2 = b2
    xa, ya = max(x1, x2), max(y1, y2)
    xb, yb = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h1)
    inter = max(0, xb - xa) * max(0, yb - ya)
    area1 = w1 * h1;
    area2 = w2 * h2
    union = area1 + area2 - inter + 1e-6
    return inter / union


def merge_into_acc(acc_list, det, iou_thr=0.5, ema=0.5):
    for a in acc_list:
        if bbox_iou(a["bbox"], det["bbox"]) > iou_thr:
            ax, ay, aw, ah = a["bbox"];
            dx, dy, dw, dh = det["bbox"]
            a["bbox"] = (int((1 - ema) * ax + ema * dx),
                         int((1 - ema) * ay + ema * dy),
                         int((1 - ema) * aw + ema * dw),
                         int((1 - ema) * ah + ema * dh))
            a["conf"] = max(a["conf"], det["conf"])
            ax, ay, az = a["xyz_cam"];
            dx, dy, dz = det["xyz_cam"]
            a["xyz_cam"] = ((1 - ema) * ax + ema * dx,
                            (1 - ema) * ay + ema * dy,
                            (1 - ema) * az + ema * dz)
            return
    acc_list.append(det)


# =============== 2 ç§’æ£€æµ‹ + æ—¶åºå¤šæ•°æŠ•ç¥¨ ===============

def collect_points_sorted(pipeline, align_filter, duration_sec=2.0):
    """
    è¿”å›æ¯ç±»è·¨å¸§åˆå¹¶åçš„å®ä¾‹ï¼ˆæŒ‰ conf é™åºï¼‰ï¼š
    { 1:[{bbox,uv,conf,xyz_cam}], 2:[...], 3:[...] }
    """
    acc = {1: [], 2: [], 3: []}
    t0 = time.time()

    while time.time() - t0 < duration_sec:
        frames = pipeline.wait_for_frames(100)
        if frames is None:
            continue

        align_filter.process(frames)
        color_frame = frames.get_color_frame()
        depth_frame = frames.get_depth_frame()
        if color_frame is None or depth_frame is None:
            continue

        cw, ch = color_frame.get_width(), color_frame.get_height()
        color = convert_color_frame_to_bgr(color_frame)
        if color is None:
            continue
        if color.shape[1] != cw or color.shape[0] != ch:
            color = cv2.resize(color, (cw, ch))

        dw, dh = depth_frame.get_width(), depth_frame.get_height()
        depth_u16 = np.frombuffer(depth_frame.get_data(), np.uint16).reshape(dh, dw)
        if (dw, dh) != (cw, ch):
            depth_u16 = cv2.resize(depth_u16.astype(np.float32), (cw, ch),
                                   interpolation=cv2.INTER_NEAREST).astype(np.uint16)
        depth = depth_u16.astype(np.float32)

        intr = color_frame.get_stream_profile().as_video_stream_profile().get_intrinsic()
        scale = float(depth_frame.get_depth_scale())

        img = cv2.resize(color, (INPUT_SIZE, INPUT_SIZE)).astype(np.float32) / 255.0
        img = np.transpose(img, (2, 0, 1))[None]
        img = torch.from_numpy(img).to(DEVICE)
        with torch.no_grad():
            out = SEG_MODEL(img)["out"][0]
            prob_small = torch.softmax(out, dim=0).cpu().numpy()
        prob = np.stack([cv2.resize(prob_small[c], (cw, ch), cv2.INTER_LINEAR)
                         for c in range(NUM_CLASSES)], axis=0)
        pred = np.argmax(prob, axis=0).astype(np.uint8)

        frame_dets = {1: [], 2: [], 3: []}
        for cls in (1, 2, 3):
            mask = (pred == cls).astype(np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for cnt in contours:
                area = cv2.contourArea(cnt)
                if area < CLS_AREA[cls]:
                    continue

                x, y, w, h = cv2.boundingRect(cnt)
                inst = np.zeros_like(mask)
                cv2.drawContours(inst, [cnt], -1, 1, -1)
                mean_conf = float((prob[cls] * inst).sum() / (inst.sum() + 1e-6))
                if mean_conf < CLS_THRESH[cls]:
                    continue

                u = x + w // 2;
                v = y + h // 2
                region = depth[max(v - 2, 0):min(v + 3, ch), max(u - 2, 0):min(u + 3, cw)]
                region = region[region > 0]
                if region.size == 0:
                    continue

                Z_mm = float(region.mean()) * scale
                X_mm = (u - intr.cx) * Z_mm / intr.fx
                Y_mm = (v - intr.cy) * Z_mm / intr.fy

                det = {
                    "bbox": (x, y, w, h),
                    "uv": (u, v),
                    "conf": mean_conf,
                    "xyz_cam": (X_mm / 1000.0, Y_mm / 1000.0, Z_mm / 1000.0)
                }

                cx, cy = u, v
                slots = TEMPORAL_BUF[cls]
                key = None
                min_d, best_k = 1e9, None
                for k, (px, py, hist) in slots.items():
                    d = (px - cx) ** 2 + (py - cy) ** 2
                    if d < min_d:
                        min_d, best_k = d, k
                if min_d < (max(w, h) * 0.8) ** 2:
                    key = best_k
                else:
                    if len(slots) >= MAX_TRACKS_PER_CLASS:
                        oldest = list(slots.keys())[0]
                        slots.pop(oldest, None)
                    key = (time.time(), cx, cy)
                if key not in slots:
                    slots[key] = (cx, cy, deque(maxlen=HISTORY))
                _, _, hist = slots[key]
                slots[key] = (cx, cy, hist)
                hist.append(1)

                if sum(hist) < VOTE_K and len(hist) >= VOTE_K:
                    continue

                frame_dets[cls].append(det)

        # ç±»é—´äº’æ–¥
        def iou(b1, b2):
            x1, y1, w1, h1 = b1;
            x2, y2, w2, h2 = b2
            xa, ya = max(x1, x2), max(y1, y2)
            xb, yb = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h1)
            inter = max(0, xb - xa) * max(0, yb - ya)
            u = w1 * h1 + w2 * h2 - inter + 1e-6
            return inter / u

        flat = []
        for c in (1, 2, 3):
            for d in frame_dets[c]:
                flat.append((CLASS_PRIORITY[c], d["conf"], c, d))
        flat.sort(key=lambda x: (x[0], x[1]), reverse=True)

        kept = []
        for _, _, c, d in flat:
            ok = True
            for kc, kd in kept:
                if iou(d["bbox"], kd["bbox"]) > 0.5 and CLASS_PRIORITY[kc] >= CLASS_PRIORITY[c]:
                    ok = False
                    break
            if ok:
                kept.append((c, d))
        frame_dets = {1: [], 2: [], 3: []}
        for c, d in kept:
            frame_dets[c].append(d)

        # é¢„è§ˆ
        vis = color.copy()
        for cls in (1, 2, 3):
            for i, d in enumerate(sorted(frame_dets[cls], key=lambda t: t["conf"], reverse=True), 1):
                x, y, w, h = d["bbox"]
                cv2.rectangle(vis, (x, y), (x + w, y + h), class_colors[cls], 2)
                cv2.putText(vis, f"{class_names[cls]} #{i} {d['conf']:.2f}", (x, y - 6),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, class_colors[cls], 2)
        cv2.imshow("collecting (frame-only)", vis)
        if cv2.waitKey(1) == 27:
            break

        for cls in (1, 2, 3):
            for d in frame_dets[cls]:
                merge_into_acc(acc[cls], d, iou_thr=0.5, ema=0.5)

    for cls in (1, 2, 3):
        acc[cls].sort(key=lambda d: d["conf"], reverse=True)
    return acc


# =============== è¯» txt çš„æœ€æ–°ä¿¡å· ===============

def read_latest_signal():
    try:
        with open(TXT_PATH, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        lines = [ln.strip() for ln in lines if ln.strip()]
        if not lines:
            print("âŒ TXT æ–‡ä»¶ä¸ºç©º")
            return None
        last_line = lines[-1]
        parts = last_line.replace(',', ' ').split()
        sig = float(parts[-1])
        return sig
    except Exception as e:
        print(f"âš  è¯»å– TXT å‡ºé”™: {e}")
        return None


# =============== å•ä¸ªç›®æ ‡çš„ Z è½´ç²—/ç»†è°ƒ + æœ€ç»ˆä¿¡å· ===============

def run_z_tuning(rtde_r, rtde_c):
    """
    åœ¨å½“å‰ä½ç½®åš Z è½´ç²—/ç»†è°ƒï¼š
      - æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œå…ˆç²—è°ƒä¸€æ­¥ï¼ˆä¸ç®¡å½“å‰ signal æ˜¯å¤šå°‘ï¼‰
      - ç²—è°ƒé˜¶æ®µï¼šåªæ ¹æ®é˜ˆå€¼ THRESHOLD åˆ¤æ–­ä½•æ—¶åˆ‡åˆ°å¾®è°ƒï¼Œä¸ç”¨ Î”signal åˆ¤ç¨³
      - å¾®è°ƒé˜¶æ®µï¼šæ‰ä½¿ç”¨ |Î”signal| <= DELTA_LIMIT è¿ç»­ N_STABLE æ¬¡æ¥åˆ¤å®šåˆ°è¾¾è¡¨é¢
    åˆ¤å®šåˆ°è¾¾è¡¨é¢åï¼Œä¿æŒ FINAL_HOLD_SECONDS ç§’é‡‡é›†æœ€ç»ˆä¿¡å·ã€‚
    è¿”å›æœ€ç»ˆä¿¡å·å€¼ï¼ˆfloat æˆ– Noneï¼‰ã€‚
    """
    stable_count = 0
    total_dz = 0.0
    reached_surface = False
    final_signal = None

    # æ¯æ¬¡è°ƒç”¨éƒ½ä»â€œæœªè¿›å…¥å¾®è°ƒâ€å¼€å§‹
    in_fine_mode = False

    print("âš™ï¸ å¼€å§‹ Z è½´ç²—/ç»†è°ƒ ...")
    print(f"   é˜ˆå€¼ THRESHOLD = {THRESHOLD}")
    print(f"   ç²—è°ƒæ­¥é•¿ STEP_COARSE = {STEP_COARSE} m")
    print(f"   ç»†è°ƒæ­¥é•¿ STEP_FINE   = {STEP_FINE} m")
    print(f"   å¾®è°ƒé˜¶æ®µç¨³å®šåˆ¤å®š: |Î”signal| <= {DELTA_LIMIT}, è¿ç»­ {N_STABLE} æ¬¡")

    # å…ˆè¯»ä¸€ä¸ªåˆå§‹ä¿¡å·ä½œä¸ºåŸºå‡†ï¼Œä¸åšåˆ¤å®šï¼Œåªæ‰“å°
    last_signal = None
    last_read_time = 0.0
    while last_signal is None:
        now = time.time()
        if now - last_read_time < READ_INTERVAL:
            time.sleep(0.01)
            continue
        last_read_time = now

        s0 = read_latest_signal()
        if s0 is not None:
            last_signal = s0
            final_signal = s0
            print(f"\nğŸ“¥ åˆå§‹ä¿¡å· = {s0:.3f}ï¼ˆä»…ä½œä¸ºèµ·ç‚¹ï¼Œä¸ç”¨äºç¨³å®šåˆ¤å®šï¼‰")

    while True:
        # 1) æ ¹æ®å½“å‰é˜¶æ®µï¼Œå†³å®šè¿™ä¸€æ­¥ç”¨ç²—è°ƒè¿˜æ˜¯å¾®è°ƒ
        if in_fine_mode:
            dz = STEP_FINE
            mode = "å¾®è°ƒ"
            vel_z = VEL_Z_FINE  # ç»†è°ƒç”¨æ›´æ…¢é€Ÿåº¦
        else:
            dz = STEP_COARSE
            mode = "ç²—è°ƒ"
            vel_z = VEL_Z_COARSE  # ç²—è°ƒç”¨ä¸­ç­‰é€Ÿåº¦

        # 2) å…ˆç§»åŠ¨ï¼Œå†è¯»æ–°ä¿¡å·
        tcp = rtde_r.getActualTCPPose()
        target = tcp.copy()
        target[2] += dz

        total_dz += dz

        print(f"\næ¨¡å¼: {mode}")
        print(f"å½“å‰ Z = {tcp[2]:.4f} m â†’ æ–° Z = {target[2]:.4f} m")
        print(f"æœ¬æ¬¡ä¸‹ç§» = {dz * 1000:.2f} mmï¼Œæ€»ä¸‹ç§» = {total_dz * 1000:.2f} mm")

        # Z è°ƒæ•´ä½¿ç”¨ä¸“ç”¨é€Ÿåº¦
        rtde_c.moveL(target, vel_z, ACC_Z)

        # ç»™ä¼ æ„Ÿå™¨ä¸€ç‚¹æ—¶é—´ç¨³å®š
        time.sleep(0.1)

        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¯»æ–°ä¿¡å·
        new_signal = None
        last_read_time = 0.0
        while new_signal is None:
            now = time.time()
            if now - last_read_time < READ_INTERVAL:
                time.sleep(0.01)
                continue
            last_read_time = now

            s = read_latest_signal()
            if s is not None:
                new_signal = s

        final_signal = new_signal
        print(f"ğŸ“¥ å½“å‰ä¿¡å· = {new_signal:.3f}")

        # 3) è®¡ç®— Î”signal
        delta = new_signal - last_signal
        print(f"Î”signal = {delta:.3f}")

        # 4) é˜¶æ®µé€»è¾‘
        if in_fine_mode:
            # â€”â€” å¾®è°ƒé˜¶æ®µï¼šç”¨ Î”signal åˆ¤ç¨³ â€”â€”
            if abs(delta) <= DELTA_LIMIT:
                stable_count += 1
                print(f"  â†’ |Î”| <= {DELTA_LIMIT}, stable_count = {stable_count}")
            else:
                stable_count = 0
                print(f"  â†’ |Î”| > {DELTA_LIMIT}, stable_count é‡ç½®ä¸º 0")

            if stable_count >= N_STABLE:
                print(f"ğŸŸ¢ å¾®è°ƒé˜¶æ®µï¼šè¿ç»­ {N_STABLE} æ¬¡ Î”signal å¾ˆå°ï¼Œåˆ¤å®šå·²åˆ°è¾¾è¡¨é¢ï¼Œåœæ­¢ä¸‹æ¢")
                reached_surface = True

        else:
            # â€”â€” ç²—è°ƒé˜¶æ®µï¼šä¸åˆ¤ç¨³ï¼Œåªåˆ¤æ–­æ˜¯å¦è¦åˆ‡æ¢åˆ°å¾®è°ƒ â€”â€”
            if new_signal >= THRESHOLD:
                in_fine_mode = True
                stable_count = 0
                print(f"ğŸ” ç²—è°ƒé˜¶æ®µä¿¡å·å·²è¾¾åˆ°é˜ˆå€¼ {THRESHOLD}ï¼Œåˆ‡æ¢åˆ°ã€å¾®è°ƒé˜¶æ®µã€‘")
            else:
                print(f"ï¼ˆç²—è°ƒé˜¶æ®µï¼‰å½“å‰ signal = {new_signal:.3f} < é˜ˆå€¼ {THRESHOLD}ï¼Œç»§ç»­ç²—è°ƒ")

        last_signal = new_signal

        if reached_surface:
            break

    # 5) åˆ°è¾¾è¡¨é¢åä¿æŒä¸€æ®µæ—¶é—´ï¼Œè¯»å–æœ€ç»ˆä¿¡å·
    print(f"â± ä¿æŒå½“å‰ä½ç½® {FINAL_HOLD_SECONDS} ç§’ï¼Œé‡‡é›†æœ€ç»ˆä¿¡å· ...")
    t_end = time.time() + FINAL_HOLD_SECONDS
    while time.time() < t_end:
        sig = read_latest_signal()
        if sig is not None:
            final_signal = sig
        time.sleep(READ_INTERVAL)

    print(f"ğŸ”š Z è°ƒæ•´ç»“æŸï¼Œæœ€ç»ˆä¿¡å· â‰ˆ {final_signal}")
    return final_signal


# =============== ä¸€æ¬¡å¤šæ ·æœ¬æµ‹é‡æµç¨‹ ===============

def measure_all_samples_once(rtde_r, rtde_c, pipeline, align_filter, home_pose):
    """
    ä¸€æ¬¡å®Œæ•´æµç¨‹ï¼š
      1) ç›¸æœºè¯†åˆ« 2 ç§’ï¼›
      2) å¯¹æ¯ä¸ªç±»åˆ« sample1/2/3 å–è‹¥å¹²æœ€é«˜ç½®ä¿¡åº¦ç›®æ ‡ï¼ˆæœ€å¤š MAX_INST_PER_CLASSï¼‰ï¼›
      3) æŒ‰ç±»åˆ«é¡ºåº 1â†’2â†’3ï¼Œç±»å†…æŒ‰ç½®ä¿¡åº¦ä»é«˜åˆ°ä½ï¼š
           - moveL åˆ°è¯¥ç›®æ ‡å¯¹åº”çš„ä¼ æ„Ÿå™¨ä½ç½®ä¸Šæ–¹
           - run_z_tuning åšç²—/ç»†è°ƒ + 2 ç§’æœ€ç»ˆä¿¡å·
           - moveL å› home_pose
    """
    print("ğŸ” å¼€å§‹ 2 ç§’è¯†åˆ«ï¼Œç”¨äºå¤šæ ·æœ¬é¡ºåºæµ‹é‡ ...")
    for c in (1, 2, 3):
        TEMPORAL_BUF[c].clear()
    instances = collect_points_sorted(pipeline, align_filter)
    print("ğŸ“¦ collected:", {k: len(v) for k, v in instances.items()})

    tcp_pose0 = rtde_r.getActualTCPPose()
    T_base_tool0 = pose_to_matrix(tcp_pose0)
    R_tc = get_cam_rotation(CAM_ORI_ID)
    t_tc = np.array([0.0, 0.0, CAM_Z_DOWN])
    T_tool_cam = np.eye(4)
    T_tool_cam[:3, :3] = R_tc
    T_tool_cam[:3, 3] = t_tc
    T_base_cam0 = T_base_tool0 @ T_tool_cam

    targets = []  # {"cls": cls, "conf": conf, "xyz_base": np.array([x,y,z])}
    for cls in (1, 2, 3):
        insts = instances[cls]
        if not insts:
            continue

        top_insts = insts[:MAX_INST_PER_CLASS]
        print(f"ğŸ“Œ {class_names[cls]} æ£€æµ‹åˆ° {len(insts)} ä¸ªï¼Œå–å‰ {len(top_insts)} ä¸ªç”¨äºæµ‹é‡")

        for inst in top_insts:
            Xc, Yc, Zc = inst["xyz_cam"]
            p_cam = np.array([Xc, Yc, Zc, 1.0])
            p_base = T_base_cam0 @ p_cam
            xyz_base = p_base[:3]
            targets.append({
                "cls": cls,
                "conf": inst["conf"],
                "xyz_base": xyz_base
            })

    if not targets:
        print("âš  æ²¡æœ‰ä»»ä½•ç›®æ ‡è¢«ç¨³å®šæ£€æµ‹åˆ°ï¼Œæœ¬è½®æµ‹é‡ç»“æŸ")
        return

    # æŒ‰ç±»åˆ«é¡ºåº 1â†’2â†’3ï¼Œç±»å†…ç½®ä¿¡åº¦ä»é«˜åˆ°ä½
    targets.sort(key=lambda t: (t["cls"], -t["conf"]))

    for idx, tgt in enumerate(targets, start=1):
        cls = tgt["cls"]
        conf = tgt["conf"]
        xyz_base = tgt["xyz_base"]

        print(f"\n================= ç›®æ ‡ {idx}/{len(targets)} =================")
        print(f"ç›®æ ‡ç±»åˆ«: {class_names[cls]}   conf = {conf:.3f}")

        current_tcp = rtde_r.getActualTCPPose()
        T_now = pose_to_matrix(current_tcp)

        tool_X = T_now[:3, 0];
        tool_X /= (np.linalg.norm(tool_X) + 1e-9)
        tool_y = T_now[:3, 1];
        tool_y /= (np.linalg.norm(tool_y) + 1e-9)
        tool_z = T_now[:3, 2];
        tool_z /= (np.linalg.norm(tool_z) + 1e-9)

        # ç›®æ ‡ç‚¹ = ç‰©ä½“ä¸­å¿ƒ + å·¥å…·Yæ–¹å‘åç§» 55mm + å·¥å…·Zæ–¹å‘å®‰å…¨ä¸‹æ¢
        target_xyz = xyz_base + tool_X * SENSOR_OFFSET_X + tool_y * SENSOR_OFFSET_y + tool_z * SAFE_OFFSET

        target_pose = [
            float(target_xyz[0]),
            float(target_xyz[1]),
            float(target_xyz[2]),
            current_tcp[3], current_tcp[4], current_tcp[5]
        ]

        print("â¡ moveL åˆ°è¯¥æ ·æœ¬å¯¹åº”çš„ä¼ æ„Ÿå™¨ä½ç½®ä¸Šæ–¹ï¼š")
        print("   target_pose =", target_pose)
        rtde_c.moveL(target_pose, VEL, ACC)
        print("âœ… å·²åˆ°è¾¾è¯¥æ ·æœ¬ä¸Šæ–¹ï¼Œå¼€å§‹ Z è½´ç²—/ç»†è°ƒ ...")

        final_signal = run_z_tuning(rtde_r, rtde_c)
        print(f"âœ… {class_names[cls]} æ­¤ç›®æ ‡æµ‹é‡å®Œæˆï¼Œæœ€ç»ˆä¿¡å· â‰ˆ {final_signal}")

        print("â†© å›åˆ° home_pose ...")
        rtde_c.moveL(home_pose, VEL, ACC)
        print("âœ… å·²å›åˆ° home_pose")

    print("\nğŸ‰ æœ¬è½®å¤šæ ·æœ¬é¡ºåºæµ‹é‡å…¨éƒ¨å®Œæˆï¼")


# =============== ä¸»æµç¨‹ ===============

def main():
    # æœºå™¨äºº
    rtde_r = RTDEReceiveInterface(ROBOT_IP)
    rtde_c = RTDEControlInterface(ROBOT_IP)
    print("âœ… robot connected")
    home_pose = rtde_r.getActualTCPPose()

    # ç›¸æœº
    pipeline = ob.Pipeline()
    config = ob.Config()

    dp = pipeline.get_stream_profile_list(ob.OBSensorType.DEPTH_SENSOR) \
        .get_video_stream_profile(640, 400, ob.OBFormat.Y16, 30)
    cp = pipeline.get_stream_profile_list(ob.OBSensorType.COLOR_SENSOR) \
        .get_video_stream_profile(640, 480, ob.OBFormat.MJPG, 30)
    config.enable_stream(dp);
    config.enable_stream(cp)

    pipeline.enable_frame_sync()
    pipeline.start(config)
    print("âœ… camera started")

    align_filter = ob.AlignFilter(ob.OBStreamType.COLOR_STREAM)

    print("æ“ä½œæç¤ºï¼š")
    print("  s - ä¸€æ¬¡å®Œæ•´ã€å¤šæ ·æœ¬é¡ºåºæµ‹é‡ã€ï¼ˆè¯†åˆ«2ç§’ â†’ ä¾æ¬¡ç§»åŠ¨+ç²—/ç»†è°ƒ+æµ‹é‡ â†’ å›homeï¼‰")
    print("  r - åªå›åˆå§‹ç‚¹ï¼Œä¸åšè¯†åˆ«å’Œæµ‹é‡")
    print("  q - é€€å‡ºç¨‹åº")

    try:
        while True:
            if msvcrt.kbhit():
                ch = msvcrt.getch().decode("utf-8", errors="ignore").lower()

                if ch == 's':
                    measure_all_samples_once(rtde_r, rtde_c, pipeline, align_filter, home_pose)

                elif ch == 'r':
                    print("â†© å›åˆå§‹ç‚¹ ...")
                    rtde_c.moveL(home_pose, VEL, ACC)
                    print("âœ… å·²å›åˆå§‹ç‚¹")

                elif ch == 'q':
                    print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                    break

            time.sleep(0.01)

    finally:
        # å…³ç›¸æœº / GUI / æœºå™¨äººè„šæœ¬
        try:
            pipeline.stop()
        except:
            pass
        try:
            cv2.destroyAllWindows()
        except:
            pass
        try:
            rtde_c.stopScript()
        except:
            pass

        # ğŸ”¹ é€€å‡ºç¨‹åºæ—¶åˆ é™¤ txt æ–‡ä»¶
        try:
            if os.path.exists(TXT_PATH):
                os.remove(TXT_PATH)
                print(f"ğŸ—‘ å·²åˆ é™¤ä¿¡å·æ—¥å¿—æ–‡ä»¶: {TXT_PATH}")
            else:
                print(f"â„¹ æœªæ‰¾åˆ° TXT æ–‡ä»¶ï¼Œæ— éœ€åˆ é™¤: {TXT_PATH}")
        except Exception as e:
            print(f"âš  åˆ é™¤ TXT æ–‡ä»¶å¤±è´¥: {e}")

        print("stopped cleanly")


if __name__ == "__main__":
    main()
